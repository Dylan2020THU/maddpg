# OpenAI simple adversary using MADDPG: agent
# Dylan
# 2024.1.9

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import random

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

LR_ACTOR = 1e-4
LR_CRITIC = 1e-3
GAMMA = 0.99
TAU = 0.005
MEMORY_SIZE = 100000
BATCH_SIZE = 64


# class ReplayBuffer:
#     def __init__(self, capacity, state_dim, obs_dim,
#                  action_dim, n_agent, batch_size):
#         self.capacity = capacity
#         self.counter = 0
#         self.action_dim = action_dim
#         self.n_agent = n_agent
#         self.batch_size = batch_size
#         self.obs_dim = obs_dim
#         self.state_cap = np.empty((self.capacity, state_dim))
#         self.new_state_cap = np.empty((self.capacity, state_dim))
#         self.reward_cap = np.empty((self.capacity, self.n_agent))
#         self.done_cap = np.empty((self.capacity, self.n_agent), dtype=bool)
# 
#         self.init_actor_capacity()
# 
#     def init_actor_capacity(self):
#         self.actor_state_cap = []
#         self.actor_new_state_cap = []
#         self.actor_action_cap = []
# 
#         for agent_i in range(self.n_agent):
#             self.actor_state_cap.append(np.empty((self.capacity, self.obs_dim[agent_i])))
#             self.actor_new_state_cap.append(np.empty((self.capacity, self.obs_dim[agent_i])))
#             self.actor_action_cap.append(np.empty((self.capacity, self.action_dim)))
# 
#     def add_memo(self, raw_obs, state, action, reward, raw_next_obs, next_state, done):
#         if self.counter % self.capacity == 0 and self.counter > 0:
#             self.init_actor_capacity()
# 
#         idx = self.counter % self.capacity
# 
#         for agent_i in range(self.n_agent):
#             self.actor_state_cap[agent_i][idx] = raw_obs[agent_i]
#             self.actor_new_state_cap[agent_i][idx] = raw_next_obs[agent_i]
#             self.actor_action_cap[agent_i][idx] = action[agent_i]
# 
#         self.state_cap[idx] = state
#         self.new_state_cap[idx] = next_state
#         self.reward_cap[idx] = reward
#         self.done_cap[idx] = done
#         self.counter += 1
# 
#     def sample(self):
#         max_cap = min(self.counter, self.capacity)
# 
#         batch_sequence = np.random.choice(max_cap, self.batch_size, replace=False)
# 
#         states = self.state_cap[batch_sequence]
#         rewards = self.reward_cap[batch_sequence]
#         next_states = self.new_state_cap[batch_sequence]
#         dones = self.done_cap[batch_sequence]
# 
#         actor_states = []
#         actor_new_states = []
#         actions = []
# 
#         for agent_i in range(self.n_agent):
#             actor_states.append(self.actor_state_cap[agent_i][batch_sequence])
#             actor_new_states.append(self.actor_new_state_cap[agent_i][batch_sequence])
#             actions.append(self.actor_action_cap[agent_i][batch_sequence])
# 
#         return actor_states, states, actions, rewards, actor_new_states, next_states, dones
# 
#     def ready(self):
#         if self.counter >= self.batch_size:
#             return True
#         return False


# create a buffer for each agent
class ReplayBuffer:
    def __init__(self, capacity, obs_dim, state_dim,
                 action_dim, n_agent, batch_size):
        self.capacity = capacity
        self.obs_cap = np.empty((self.capacity, obs_dim))
        self.next_obs_cap = np.empty((self.capacity, obs_dim))
        self.state_cap = np.empty((self.capacity, state_dim))
        self.new_state_cap = np.empty((self.capacity, state_dim))
        self.action_cap = np.empty((self.capacity, action_dim), dtype=np.int8)
        self.reward_cap = np.empty((self.capacity, n_agent))
        self.done_cap = np.empty((self.capacity, n_agent), dtype=bool)

        self.batch_size = batch_size
        self.count = 0
        self.current = 0

    #     self.init_actor_capacity()
    #
    # def init_actor_capacity(self):
    #     self.actor_state_cap = []
    #     self.actor_new_state_cap = []
    #     self.actor_action_cap = []
    #
    #     for agent_i in range(self.n_agent):
    #         self.actor_state_cap.append(np.empty((self.capacity, self.obs_dim[agent_i])))
    #         self.actor_new_state_cap.append(np.empty((self.capacity, self.obs_dim[agent_i])))
    #         self.actor_action_cap.append(np.empty((self.capacity, self.action_dim)))

    def add_memo(self, obs, next_obs, state, next_state, action, reward, done):
        self.obs_cap[self.current] = obs
        self.next_obs_cap[self.current] = next_obs
        self.state_cap[self.current] = state
        self.new_state_cap[self.current] = next_state
        self.action_cap[self.current] = action
        self.reward_cap[self.current] = reward
        self.done_cap[self.current] = done
        self.count = max(self.count, self.current + 1)
        self.current = (self.current + 1) % self.capacity
        # if self.counter % self.capacity == 0 and self.counter > 0:
        #     self.init_actor_capacity()
        #
        # idx = self.counter % self.capacity
        #
        # for agent_i in range(self.n_agent):
        #     self.actor_state_cap[agent_i][idx] = raw_obs[agent_i]
        #     self.actor_new_state_cap[agent_i][idx] = raw_next_obs[agent_i]
        #     self.actor_action_cap[agent_i][idx] = action[agent_i]
        #
        # self.state_cap[idx] = state
        # self.new_state_cap[idx] = next_state
        # self.reward_cap[idx] = reward
        # self.done_cap[idx] = done
        # self.counter += 1

    def sample(self):
        if self.count < self.batch_size:
            idxes = range(0, self.count)
        else:
            idxes = random.sample(range(0, self.count), self.batch_size)

        obses = self.obs_cap[idxes]
        next_obses = self.next_obs_cap[idxes]
        states = self.state_cap[idxes]
        new_states = self.new_state_cap[idxes]
        actions = self.action_cap[idxes]
        rewards = self.reward_cap[idxes]
        dones = self.done_cap[idxes]

        return obses, next_obses, states, new_states, actions, rewards, dones
        # max_cap = min(self.counter, self.capacity)
        # 
        # batch_sequence = np.random.choice(max_cap, self.batch_size, replace=False)
        # 
        # states = self.state_cap[batch_sequence]
        # rewards = self.reward_cap[batch_sequence]
        # next_states = self.new_state_cap[batch_sequence]
        # dones = self.done_cap[batch_sequence]
        # 
        # actor_states = []
        # actor_new_states = []
        # actions = []
        # 
        # for agent_i in range(self.n_agent):
        #     actor_states.append(self.actor_state_cap[agent_i][batch_sequence])
        #     actor_new_states.append(self.actor_new_state_cap[agent_i][batch_sequence])
        #     actions.append(self.actor_action_cap[agent_i][batch_sequence])


class Critic(nn.Module):
    def __init__(self, lr_critic, input_dims, fc1_dims, fc2_dims,
                 n_agent, action_dim):
        super(Critic, self).__init__()

        self.fc1 = nn.Linear(input_dims + n_agent * action_dim, fc1_dims)
        self.fc2 = nn.Linear(fc1_dims, fc2_dims)
        self.q = nn.Linear(fc2_dims, 1)

        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr_critic)
        # self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        # self.to(self.device)  # TODO: not sure if self.device or device

    def forward(self, state, action):
        x = F.relu(self.fc1(torch.cat([state, action], dim=1)))
        x = F.relu(self.fc2(x))
        q = self.q(x)

        return q

    def save_checkpoint(self, chkpt_file):
        torch.save(self.state_dict(), chkpt_file)

    def load_checkpoint(self, chkpt_file):
        self.load_state_dict(torch.load(chkpt_file))


class Actor(nn.Module):
    def __init__(self, lr_actor, input_dims, fc1_dims, fc2_dims,
                 action_dim):
        super(Actor, self).__init__()

        self.fc1 = nn.Linear(input_dims, fc1_dims)
        self.fc2 = nn.Linear(fc1_dims, fc2_dims)
        self.pi = nn.Linear(fc2_dims, action_dim)

        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr_actor)
        # self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        # self.to(self.device)  # TODO: not sure if self.device or device

    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        mu = torch.softmax(self.pi(x), dim=1)
        return mu

    def save_checkpoint(self, chkpt_file):
        torch.save(self.state_dict(), chkpt_file)

    def load_checkpoint(self, chkpt_file):
        self.load_state_dict(torch.load(chkpt_file))


class Agent:
    def __init__(self, obs_dim, state_dim, n_agent, action_dim, agent_i,
                 alpha=LR_ACTOR, beta=LR_CRITIC, fc1_dims=64, fc2_dims=64, gamma=GAMMA, tau=TAU):
        self.gamma = gamma
        self.tau = tau
        self.action_dim = action_dim

        self.actor = Actor(lr_actor=alpha, input_dims=obs_dim, fc1_dims=fc1_dims, fc2_dims=fc2_dims,
                           action_dim=action_dim)
        self.critic = Critic(lr_critic=beta, input_dims=state_dim, fc1_dims=fc1_dims, fc2_dims=fc2_dims,
                             n_agent=n_agent, action_dim=action_dim)
        self.target_actor = Actor(lr_actor=alpha, input_dims=obs_dim, fc1_dims=fc1_dims, fc2_dims=fc2_dims,
                                  action_dim=action_dim)
        self.target_critic = Critic(lr_critic=beta, input_dims=state_dim, fc1_dims=fc1_dims, fc2_dims=fc2_dims,
                                    n_agent=n_agent, action_dim=action_dim)

        self.replay_buffer = ReplayBuffer(MEMORY_SIZE, obs_dim, state_dim, action_dim, n_agent, batch_size=BATCH_SIZE)

    #     self.update_network(tau=1)
    #
    # def update_network(self, tau=None):
    #     if tau == None:
    #         tau = self.tau
    #
    #     # Actor network
    #     actor_nn_dict = dict(self.actor.named_parameters())
    #     target_actor_nn_dict = dict(self.target_actor.named_parameters())
    #
    #     for param in actor_nn_dict:
    #         actor_nn_dict[param] = tau * actor_nn_dict[param].clone() + (1 - tau) * target_actor_nn_dict[
    #             param].clone()
    #
    #     self.target_actor.load_state_dict(actor_nn_dict)
    #
    #     # Critic network
    #     critic_nn_dict = dict(self.critic.named_parameters())
    #     target_critic_nn_dict = dict(self.target_critic.named_parameters())
    #
    #     for param in critic_nn_dict:
    #         critic_nn_dict[param] = tau * critic_nn_dict[param].clone() + (1 - tau) * target_critic_nn_dict[
    #             param].clone()
    #
    #     self.target_critic.load_state_dict(critic_nn_dict)

    def get_action(self, observation):
        state = torch.tensor(data=[observation], dtype=torch.float).to(device)
        actions = self.actor.forward(state)
        noise = torch.rand(self.action_dim).to(device)
        action = actions + noise
        return action.detach().cpu().numpy()[0]

    def save_model(self, filename):
        self.actor.save_checkpoint(filename)
        self.target_actor.save_checkpoint(filename)
        self.critic.save_checkpoint(filename)
        self.target_critic.save_checkpoint(filename)

    def load_model(self, filename):
        self.actor.load_checkpoint(filename)
        self.target_actor.load_checkpoint(filename)
        self.critic.load_checkpoint(filename)
        self.target_critic.load_checkpoint(filename)
    #
    # # Each agent starts learning
    # def learn(self):
    #     if self.replay_buffer.current < self.replay_buffer.batch_size:
    #         return
    #
    #     batch_obses, batch_next_obses, batch_states, batch_new_states, batch_actions, batch_rewards, batch_dones = self.replay_buffer.sample()
    #     device = self.actor.device
    #
    #     batch_obses_tensor = torch.tensor(batch_obses, dtype=torch.float).to(device)
    #     batch_next_obses_tensor = torch.tensor(batch_next_obses, dtype=torch.float).to(device)
    #     batch_states_tensor = torch.tensor(batch_states, dtype=torch.float).to(device)
    #     batch_new_states_tensor = torch.tensor(batch_new_states, dtype=torch.float).to(device)
    #     batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.float).to(device)
    #     batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float).to(device)
    #     batch_dones_tensor = torch.tensor(batch_dones).to(device)
    #
    #     # Update critic and actor
    #     # Calculate new actions using target actor
    #     single_new_action = self.target_actor.forward(batch_next_obses_tensor)
    #     # Calculate target Q value using target critic
    #     critic_target_value = self.target_critic.forward(batch_new_states_tensor, single_new_action).flatten()
    #
    #     all_agents_new_actions = []
    #     all_agents_new_mu_actions = []
    #     old_agents_actions = []
    #
    #     for agent_i, agent in enumerate(self.agents):
    #         tmp = torch.tensor(actor_new_states[agent_i], dtype=torch.float).to(device)
    #         new_pi = agent.target_actor.forward(tmp)
    #         all_agents_new_actions.append(new_pi)
    #
    #         mu_states = torch.tensor(actor_states[agent_i], dtype=torch.float).to(device)
    #         pi = agent.actor.forward(mu_states)
    #         all_agents_new_mu_actions.append(pi)
    #
    #         old_agents_actions.append(actions[agent_i])
    #
    #     new_actions = torch.cat([acts for acts in all_agents_new_actions], dim=1)
    #     mu = torch.cat([acts for acts in all_agents_new_mu_actions], dim=1)
    #     old_actions = torch.cat([acts for acts in old_agents_actions], dim=1)
    #
    #     for agent_i, agent in enumerate(self.agents):
    #         critic_target_value = agent.target_critic.forward(new_states, new_actions).flatten()
    #         critic_target_value[dones[:, 0]] = 0.0
    #         critic_value = agent.critic.forward(states, old_actions).flatten()
    #
    #         target = rewards[:, agent_i] + agent.gamma * critic_target_value
    #         critic_loss = F.mse_loss(target, critic_value)
    #         agent.critic.optimizer.zero_grad()



# class MADDPG:
#     def __init__(self, obs_dim, state_dim, action_dim, n_agent,
#                  alpha, beta, fc1_dims, fc2_dims, gamma, tau, agent_param_path):
#         # TODO: Initialize actors and critics
#         self.agents = []
#         self.n_agent = n_agent
#         self.action_dim = action_dim
#
#
#
#
#
#         # self.agents = []
#         # self.n_agent = n_agent
#         # self.action_dim = action_dim
#         #
#         # for agent_i in range(self.n_agent):
#         #     self.agents.append(Agent(obs_dim=obs_dim[agent_i], state_dim=state_dim, n_agent=self.n_agent,
#         #                              action_dim=action_dim, agent_i=agent_i, alpha=alpha, beta=beta,
#         #                              agent_param_path=agent_param_path))
#         # self.replay_buffer = ReplayBuffer(MEMORY_SIZE, obs_dim, state_dim, action_dim, n_agent, batch_size=BATCH_SIZE)
#
#     def save_checkpoint(self):
#         print("Saving parameters...")
#         for agent_i in self.agents:
#             agent_i.save_model()
#
#     def load_checkpoint(self):
#         print("Loading parameters...")
#         for agent_i in self.agents:
#             agent_i.load_models()
#
#     def get_actions(self, raw_obs):
#         actions = []
#         for agent_i, agent in enumerate(self.agents):
#             action = agent.get_action(raw_obs[agent_i])
#             actions.append(action)
#
#         return actions
#
#     def learn(self, memory):
#         if self.replay_buffer.current < self.replay_buffer.batch_size:
#             return
#
#         obses, next_obses, states, new_states, actions, rewards, dones = memory.sample()
#         device = self.agents[0].actor.device
#
#         states = torch.tensor(states, dtype=torch.float).to(device)
#         actions = torch.tensor(actions, dtype=torch.float).to(device)
#         rewards = torch.tensor(rewards, dtype=torch.float).to(device)
#         new_states = torch.tensor(new_states, dtype=torch.float).to(device)
#         dones = torch.tensor(dones).to(device)
#
#         all_agents_new_actions = []
#         all_agents_new_mu_actions = []
#         old_agents_actions = []
#
#         for agent_i, agent in enumerate(self.agents):
#             tmp = torch.tensor(actor_new_states[agent_i], dtype=torch.float).to(device)
#             new_pi = agent.target_actor.forward(tmp)
#             all_agents_new_actions.append(new_pi)
#
#             mu_states = torch.tensor(actor_states[agent_i], dtype=torch.float).to(device)
#             pi = agent.actor.forward(mu_states)
#             all_agents_new_mu_actions.append(pi)
#
#             old_agents_actions.append(actions[agent_i])
#
#         new_actions = torch.cat([acts for acts in all_agents_new_actions], dim=1)
#         mu = torch.cat([acts for acts in all_agents_new_mu_actions], dim=1)
#         old_actions = torch.cat([acts for acts in old_agents_actions], dim=1)
#
#         for agent_i, agent in enumerate(self.agents):
#             critic_target_value = agent.target_critic.forward(new_states, new_actions).flatten()
#             critic_target_value[dones[:, 0]] = 0.0
#             critic_value = agent.critic.forward(states, old_actions).flatten()
#
#             target = rewards[:, agent_i] + agent.gamma * critic_target_value
#             critic_loss = F.mse_loss(target, critic_value)
#             agent.critic.optimizer.zero_grad()  # .zero_grad() clears old gradients from the last step
#             # critic_loss.backward(retain_graph=True)
#             critic_loss.backward()  # .backward() computes the derivative of the loss
#             agent.critic.optimizer.step()  # .step() is to update the parameters
#
#             actor_loss = agent.critic.forward(states, mu).flatten()
#             actor_loss = -torch.mean(actor_loss)
#             agent.actor.optimizer.zero_grad()
#             # actor_loss.backward(retain_graph=True)
#             actor_loss.backward()
#             agent.actor.optimizer.step()
#             print(f"Agent: {agent_i}, Critic Loss: {critic_loss}, Actor Loss: {actor_loss}")
#
#             agent.update_network()
